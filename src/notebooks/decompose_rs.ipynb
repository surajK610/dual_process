{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2e2a33-4b92-4636-9504-193915148bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/sanand14/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /oscar/home/sanand14/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator12recordStreamERKNS_7DataPtrENS0_10CUDAStreamE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import logging\n",
    "from typing import cast, Dict, List, Tuple, Union\n",
    "from typing_extensions import get_args, Literal\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append('/home/src/experiments/utils')\n",
    "sys.path.append('/home/src/experiments')\n",
    "\n",
    "from utils.probing_utils import AccuracyProbe\n",
    "from utils.data_utils import makeHooks, decomposeHeads, decomposeSingleHead\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformer_lens import HookedEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71540d-fd7f-44ba-a841-e8d811e70526",
   "metadata": {},
   "source": [
    "# CHECK RESID STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c985f8-5721-48fd-9b4b-0460e03e11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for BERT in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using BERT for interpretability research, keep in mind that BERT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Loaded pretrained model bert-base-cased into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "model_hooked = HookedEncoder.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5283b-ec17-47fd-a3a5-ad418a99a241",
   "metadata": {},
   "source": [
    "### Attention Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adcfb33-2481-4e90-9f7a-885f40b3435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHooks(model, cache : defaultdict, remove_batch_dim=False, device='cpu'):\n",
    "    def hook_self_attention(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if remove_batch_dim:\n",
    "            cache['attention'].append(output[0].detach().to(device))\n",
    "        else:\n",
    "            cache['attention'].append(output.detach().to(device))\n",
    "\n",
    "    def hook_attn_output(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if remove_batch_dim:\n",
    "            cache['attention_out'].append(output[0].detach().to(device))\n",
    "        else:\n",
    "            cache['attention_out'].append(output.detach().to(device))\n",
    "            \n",
    "    for layer in model.encoder.layer:\n",
    "        layer.attention.self.register_forward_hook(hook_self_attention)\n",
    "        \n",
    "    for layer in model.encoder.layer:\n",
    "        layer.attention.output.dense.register_forward_hook(hook_attn_output)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3170681b-b8b0-4bc3-afcb-e15828e56905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposeHeads(model, attention_vectors):\n",
    "    \"\"\"\n",
    "    Decompose attention heads into subspaces.\n",
    "    `(cache['attention'][0] @ model.encoder.layer[0].attention.output.dense.weight.data.T) + model.encoder.layer[0].attention.output.dense.bias`\n",
    "    \"\"\"\n",
    "    attention_head_dict = {}\n",
    "    assert len(attention_vectors) == 12\n",
    "    for i, attn_layer in enumerate(tqdm(attention_vectors, desc='Decomposing attention heads')):\n",
    "        output_matrix = model.encoder.layer[i].attention.output.dense.weight.data.T\n",
    "        for j in range(model.config.num_attention_heads):\n",
    "            output_slice = output_matrix[j*64:(j+1)*64, :]\n",
    "            if len(attn_layer.shape) == 2:\n",
    "                attn_slice = attn_layer[:, j*64:(j+1)*64]\n",
    "            elif len(attn_layer.shape) == 3:\n",
    "                attn_slice = attn_layer[:, :, j*64:(j+1)*64] \n",
    "                ## if batch dim intact\n",
    "            else:\n",
    "                raise ValueError('Attention layer has unexpected shape')\n",
    "            \n",
    "            attention_head_dict[AttnHead(i, j)] =  attn_slice @ output_slice\n",
    "    return attention_head_dict\n",
    "\n",
    "def decomposeSingleHead(model, attention_vector, layer, head):\n",
    "    \"\"\"\n",
    "    Decompose attention heads into subspaces.\n",
    "    layer is 1-indexed so use layer-1\n",
    "    \"\"\"\n",
    "    attention_head_dict = {}\n",
    "    output_matrix = model.encoder.layer[layer-1].attention.output.dense.weight.data.T\n",
    "    output_slice = output_matrix[head*64:(head+1)*64, :]\n",
    "    if len(attention_vector.shape) == 2:\n",
    "        attn_slice = attention_vector[:, head*64:(head+1)*64]\n",
    "    elif len(attention_vector.shape) == 3:\n",
    "        attn_slice = attention_vector[:, :, head*64:(head+1)*64] \n",
    "        ## if batch dim intact\n",
    "    else:\n",
    "        raise ValueError('Attention layer has unexpected shape')\n",
    "    return attn_slice @ output_slice.cpu().numpy()\n",
    "\n",
    "def getAttnOMatrix(model, layer):\n",
    "    output_matrix = model.encoder.layer[layer].attention.output.dense.weight.data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2155ac67-5d1d-4e1e-9275-e97cc16caf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = defaultdict(list)\n",
    "cache = makeHooks(model, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0a849-b1b2-4531-814e-9cc80f4b9e9e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb0a202a-4ab5-4d61-b41e-a91c3600f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "token_example = torch.tensor([[101, 40, 23, 12, 34, 2, 103, 4323, 12, 102]])\n",
    "_ = model(token_example)\n",
    "_, cache_h = model_hooked.run_with_cache(token_example)\n",
    "out_head = cache_h.stack_head_results(layer=1, return_labels=True, incl_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6301d425-272b-4f6e-a59e-0cf8eb1b6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_head_dict = {val: ten for ten, val in zip(out_head[0], out_head[1])}\n",
    "attn_out_l0 = (cache_h['blocks.0.attn.hook_result'].sum(axis=2) + model.encoder.layer[0].attention.output.dense.bias)\n",
    "calc_l0_h0 = decomposeSingleHead(model, cache['attention'][0].detach().numpy(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f281bf83-126d-44f9-a2fd-30ba5ab53b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(cache_h['blocks.0.attn.hook_result'][:, :, 0, :] == out_head_dict['L0H0'])\n",
    "assert np.isclose(cache_h['blocks.0.attn.hook_result'][:, :, 0, :].detach().numpy(), calc_l0_h0, 1e-4).mean()  > 0.98\n",
    "assert np.isclose(attn_out.detach().numpy(), cache_h['blocks.0.hook_attn_out'].detach().numpy(), 1e-4).mean() > 0.98\n",
    "assert np.isclose(cache['attention_out'][0], cache_h['blocks.0.hook_attn_out'].detach().numpy(), 1e-4).mean() > 0.98\n",
    "\n",
    "## making sure all ways to calculate heads and attention out are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a8a5a600-62a4-4377-a4c1-987f610c5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "AttnHead = namedtuple(\"AttnHead\", \"layer head\")\n",
    "heads_decomp = decomposeHeads(model, cache['attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3e88ba7d-3857-4982-8cd0-0b0cd4bbc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_head_all = cache_h.stack_head_results(layer=-1, return_labels=True, incl_remainder=False)\n",
    "out_head_all_dict = {val: ten for ten, val in zip(out_head_all[0], out_head_all[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d4dea3-386a-4a05-a369-0616e511e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attn ones were messed up by 0-indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7512c4d-e4da-4363-81ea-1cc5e939659a",
   "metadata": {},
   "source": [
    "# FIX ATTENTION PROBING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de65cbc-0490-45ef-bb40-ba51b03a0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aheads import generate_algo_task_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb35a8-bda4-4b63-ba84-03fd2a1b4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_algo_task_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682e36e-c539-4ed4-b624-1e83b61ccdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab1837-8b1a-40c1-aa0f-157f599a7532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
